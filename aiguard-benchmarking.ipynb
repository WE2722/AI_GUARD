{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T01:26:54.546884Z",
     "iopub.status.busy": "2025-05-13T01:26:54.546179Z",
     "iopub.status.idle": "2025-05-13T01:27:08.832937Z",
     "shell.execute_reply": "2025-05-13T01:27:08.832110Z",
     "shell.execute_reply.started": "2025-05-13T01:26:54.546855Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T01:27:08.834520Z",
     "iopub.status.busy": "2025-05-13T01:27:08.834203Z",
     "iopub.status.idle": "2025-05-13T01:27:08.932603Z",
     "shell.execute_reply": "2025-05-13T01:27:08.931968Z",
     "shell.execute_reply.started": "2025-05-13T01:27:08.834500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T01:27:08.933724Z",
     "iopub.status.busy": "2025-05-13T01:27:08.933486Z",
     "iopub.status.idle": "2025-05-13T01:27:09.667726Z",
     "shell.execute_reply": "2025-05-13T01:27:09.666890Z",
     "shell.execute_reply.started": "2025-05-13T01:27:08.933707Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded to: /kaggle/input/aiguard-split-data\n",
      "Contents of dataset folder: ['split_data']\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "\n",
    "# Re-download the dataset if necessary\n",
    "path = kagglehub.dataset_download(\"wiameelhafid/aiguard-split-data\")\n",
    "print(\"Dataset downloaded to:\", path)\n",
    "print(\"Contents of dataset folder:\", os.listdir(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T01:27:09.669907Z",
     "iopub.status.busy": "2025-05-13T01:27:09.669649Z",
     "iopub.status.idle": "2025-05-13T01:27:09.674762Z",
     "shell.execute_reply": "2025-05-13T01:27:09.674018Z",
     "shell.execute_reply.started": "2025-05-13T01:27:09.669887Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of dataset folder: ['split_data']\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/kaggle/input/aiguard-split-data\"\n",
    "print(\"Contents of dataset folder:\", os.listdir(dataset_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T01:27:09.676372Z",
     "iopub.status.busy": "2025-05-13T01:27:09.675597Z",
     "iopub.status.idle": "2025-05-13T01:27:09.701634Z",
     "shell.execute_reply": "2025-05-13T01:27:09.701025Z",
     "shell.execute_reply.started": "2025-05-13T01:27:09.676346Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of dataset folder: ['val', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import datasets\n",
    "\n",
    "# Correct dataset path\n",
    "dataset_path = \"/kaggle/input/aiguard-split-data/split_data\"\n",
    "\n",
    "# Check contents to confirm\n",
    "print(\"Contents of dataset folder:\", os.listdir(dataset_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T01:27:09.702518Z",
     "iopub.status.busy": "2025-05-13T01:27:09.702318Z",
     "iopub.status.idle": "2025-05-13T01:27:09.717548Z",
     "shell.execute_reply": "2025-05-13T01:27:09.716868Z",
     "shell.execute_reply.started": "2025-05-13T01:27:09.702497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),  # Convert images to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T01:27:09.718978Z",
     "iopub.status.busy": "2025-05-13T01:27:09.718781Z",
     "iopub.status.idle": "2025-05-13T01:27:09.770411Z",
     "shell.execute_reply": "2025-05-13T01:27:09.769782Z",
     "shell.execute_reply.started": "2025-05-13T01:27:09.718962Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 626\n",
      "Number of validation samples: 626\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a custom Dataset class for .npy files\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.data_files = sorted(os.listdir(data_dir))  # Assuming sorted order corresponds to labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.data_dir, self.data_files[idx])\n",
    "        data = np.load(file_path, allow_pickle=True).item()  # Assuming saved as dict with 'image' and 'label'\n",
    "        image = data['image']\n",
    "        label = data['label']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Dataset paths\n",
    "train_path = \"/kaggle/input/aiguard-split-data/split_data/train\"\n",
    "val_path = \"/kaggle/input/aiguard-split-data/split_data/val\"\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert numpy array to PyTorch tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = NumpyDataset(train_path, transform=transform)\n",
    "val_dataset = NumpyDataset(val_path, transform=transform)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Print dataset details\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T01:27:09.771386Z",
     "iopub.status.busy": "2025-05-13T01:27:09.771160Z",
     "iopub.status.idle": "2025-05-13T01:27:10.538282Z",
     "shell.execute_reply": "2025-05-13T01:27:10.537472Z",
     "shell.execute_reply.started": "2025-05-13T01:27:09.771366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.13.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model, loss function, and optimizer\n",
    "model = torch.hub.load(\"pytorch/vision:v0.13.1\", \"resnet18\", weights=\"IMAGENET1K_V1\")\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, 10)  # Update for 10 classes (adjust based on your dataset)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "train_losses, train_accuracies = [], []\n",
    "val_losses, val_accuracies = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T01:27:10.539436Z",
     "iopub.status.busy": "2025-05-13T01:27:10.539157Z",
     "iopub.status.idle": "2025-05-13T01:27:10.545173Z",
     "shell.execute_reply": "2025-05-13T01:27:10.544378Z",
     "shell.execute_reply.started": "2025-05-13T01:27:10.539410Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train directory contents: ['train_batch_118.npy', 'train_batch_273.npy', 'train_batch_119.npy', 'train_batch_35.npy', 'train_batch_282.npy', 'train_batch_154.npy', 'train_batch_523.npy', 'train_batch_120.npy', 'train_batch_493.npy', 'train_batch_415.npy', 'train_batch_449.npy', 'train_batch_346.npy', 'train_batch_469.npy', 'train_batch_564.npy', 'train_batch_495.npy', 'train_batch_552.npy', 'train_batch_19.npy', 'train_batch_543.npy', 'train_batch_529.npy', 'train_batch_606.npy', 'train_batch_539.npy', 'train_batch_201.npy', 'train_batch_497.npy', 'train_batch_522.npy', 'train_batch_554.npy', 'train_batch_116.npy', 'train_batch_9.npy', 'train_batch_57.npy', 'train_batch_618.npy', 'train_batch_182.npy', 'train_batch_350.npy', 'train_batch_365.npy', 'train_batch_341.npy', 'train_batch_134.npy', 'train_batch_388.npy', 'train_batch_541.npy', 'train_batch_598.npy', 'train_batch_555.npy', 'train_batch_357.npy', 'train_batch_610.npy', 'train_batch_488.npy', 'train_batch_337.npy', 'train_batch_520.npy', 'train_batch_609.npy', 'train_batch_418.npy', 'train_batch_177.npy', 'train_batch_586.npy', 'train_batch_473.npy', 'train_batch_540.npy', 'train_batch_518.npy', 'train_batch_44.npy', 'train_batch_490.npy', 'train_batch_281.npy', 'train_batch_193.npy', 'train_batch_233.npy', 'train_batch_40.npy', 'train_batch_332.npy', 'train_batch_283.npy', 'train_batch_611.npy', 'train_batch_103.npy', 'train_batch_413.npy', 'train_batch_92.npy', 'train_batch_531.npy', 'train_batch_369.npy', 'train_batch_125.npy', 'train_batch_223.npy', 'train_batch_213.npy', 'train_batch_158.npy', 'train_batch_48.npy', 'train_batch_255.npy', 'train_batch_385.npy', 'train_batch_238.npy', 'train_batch_150.npy', 'train_batch_152.npy', 'train_batch_110.npy', 'train_batch_52.npy', 'train_batch_392.npy', 'train_batch_333.npy', 'train_batch_516.npy', 'train_batch_361.npy', 'train_batch_277.npy', 'train_batch_625.npy', 'train_batch_174.npy', 'train_batch_190.npy', 'train_batch_81.npy', 'train_batch_447.npy', 'train_batch_411.npy', 'train_batch_157.npy', 'train_batch_597.npy', 'train_batch_76.npy', 'train_batch_336.npy', 'train_batch_179.npy', 'train_batch_200.npy', 'train_batch_226.npy', 'train_batch_163.npy', 'train_batch_370.npy', 'train_batch_79.npy', 'train_batch_122.npy', 'train_batch_171.npy', 'train_batch_212.npy', 'train_batch_367.npy', 'train_batch_27.npy', 'train_batch_335.npy', 'train_batch_296.npy', 'train_batch_209.npy', 'train_batch_112.npy', 'train_batch_194.npy', 'train_batch_101.npy', 'train_batch_196.npy', 'train_batch_307.npy', 'train_batch_508.npy', 'train_batch_352.npy', 'train_batch_578.npy', 'train_batch_285.npy', 'train_batch_548.npy', 'train_batch_437.npy', 'train_batch_139.npy', 'train_batch_331.npy', 'train_batch_16.npy', 'train_batch_197.npy', 'train_batch_311.npy', 'train_batch_560.npy', 'train_batch_186.npy', 'train_batch_587.npy', 'train_batch_269.npy', 'train_batch_504.npy', 'train_batch_253.npy', 'train_batch_355.npy', 'train_batch_599.npy', 'train_batch_362.npy', 'train_batch_263.npy', 'train_batch_320.npy', 'train_batch_457.npy', 'train_batch_244.npy', 'train_batch_573.npy', 'train_batch_475.npy', 'train_batch_456.npy', 'train_batch_99.npy', 'train_batch_148.npy', 'train_batch_130.npy', 'train_batch_187.npy', 'train_batch_364.npy', 'train_batch_390.npy', 'train_batch_265.npy', 'train_batch_65.npy', 'train_batch_570.npy', 'train_batch_25.npy', 'train_batch_569.npy', 'train_batch_591.npy', 'train_batch_128.npy', 'train_batch_258.npy', 'train_batch_403.npy', 'train_batch_494.npy', 'train_batch_185.npy', 'train_batch_537.npy', 'train_batch_430.npy', 'train_batch_324.npy', 'train_batch_481.npy', 'train_batch_98.npy', 'train_batch_318.npy', 'train_batch_542.npy', 'train_batch_78.npy', 'train_batch_476.npy', 'train_batch_343.npy', 'train_batch_556.npy', 'train_batch_6.npy', 'train_batch_600.npy', 'train_batch_500.npy', 'train_batch_206.npy', 'train_batch_219.npy', 'train_batch_60.npy', 'train_batch_549.npy', 'train_batch_445.npy', 'train_batch_407.npy', 'train_batch_165.npy', 'train_batch_61.npy', 'train_batch_220.npy', 'train_batch_271.npy', 'train_batch_261.npy', 'train_batch_64.npy', 'train_batch_620.npy', 'train_batch_454.npy', 'train_batch_94.npy', 'train_batch_155.npy', 'train_batch_562.npy', 'train_batch_526.npy', 'train_batch_172.npy', 'train_batch_54.npy', 'train_batch_236.npy', 'train_batch_359.npy', 'train_batch_623.npy', 'train_batch_276.npy', 'train_batch_416.npy', 'train_batch_595.npy', 'train_batch_396.npy', 'train_batch_381.npy', 'train_batch_502.npy', 'train_batch_544.npy', 'train_batch_327.npy', 'train_batch_229.npy', 'train_batch_147.npy', 'train_batch_485.npy', 'train_batch_553.npy', 'train_batch_143.npy', 'train_batch_455.npy', 'train_batch_514.npy', 'train_batch_3.npy', 'train_batch_214.npy', 'train_batch_151.npy', 'train_batch_316.npy', 'train_batch_344.npy', 'train_batch_305.npy', 'train_batch_270.npy', 'train_batch_2.npy', 'train_batch_442.npy', 'train_batch_73.npy', 'train_batch_202.npy', 'train_batch_95.npy', 'train_batch_199.npy', 'train_batch_56.npy', 'train_batch_423.npy', 'train_batch_222.npy', 'train_batch_70.npy', 'train_batch_590.npy', 'train_batch_482.npy', 'train_batch_115.npy', 'train_batch_252.npy', 'train_batch_247.npy', 'train_batch_527.npy', 'train_batch_288.npy', 'train_batch_203.npy', 'train_batch_356.npy', 'train_batch_426.npy', 'train_batch_59.npy', 'train_batch_328.npy', 'train_batch_363.npy', 'train_batch_93.npy', 'train_batch_63.npy', 'train_batch_317.npy', 'train_batch_339.npy', 'train_batch_46.npy', 'train_batch_601.npy', 'train_batch_142.npy', 'train_batch_204.npy', 'train_batch_507.npy', 'train_batch_583.npy', 'train_batch_425.npy', 'train_batch_274.npy', 'train_batch_409.npy', 'train_batch_617.npy', 'train_batch_191.npy', 'train_batch_32.npy', 'train_batch_21.npy', 'train_batch_511.npy', 'train_batch_275.npy', 'train_batch_166.npy', 'train_batch_11.npy', 'train_batch_62.npy', 'train_batch_342.npy', 'train_batch_30.npy', 'train_batch_71.npy', 'train_batch_451.npy', 'train_batch_459.npy', 'train_batch_572.npy', 'train_batch_156.npy', 'train_batch_23.npy', 'train_batch_432.npy', 'train_batch_140.npy', 'train_batch_82.npy', 'train_batch_141.npy', 'train_batch_90.npy', 'train_batch_126.npy', 'train_batch_462.npy', 'train_batch_42.npy', 'train_batch_266.npy', 'train_batch_312.npy', 'train_batch_259.npy', 'train_batch_22.npy', 'train_batch_136.npy', 'train_batch_422.npy', 'train_batch_498.npy', 'train_batch_228.npy', 'train_batch_384.npy', 'train_batch_170.npy', 'train_batch_12.npy', 'train_batch_137.npy', 'train_batch_506.npy', 'train_batch_4.npy', 'train_batch_13.npy', 'train_batch_287.npy', 'train_batch_558.npy', 'train_batch_383.npy', 'train_batch_43.npy', 'train_batch_414.npy', 'train_batch_439.npy', 'train_batch_394.npy', 'train_batch_131.npy', 'train_batch_132.npy', 'train_batch_424.npy', 'train_batch_474.npy', 'train_batch_18.npy', 'train_batch_184.npy', 'train_batch_231.npy', 'train_batch_183.npy', 'train_batch_464.npy', 'train_batch_129.npy', 'train_batch_100.npy', 'train_batch_483.npy', 'train_batch_410.npy', 'train_batch_109.npy', 'train_batch_292.npy', 'train_batch_162.npy', 'train_batch_528.npy', 'train_batch_484.npy', 'train_batch_622.npy', 'train_batch_24.npy', 'train_batch_145.npy', 'train_batch_31.npy', 'train_batch_242.npy', 'train_batch_605.npy', 'train_batch_192.npy', 'train_batch_303.npy', 'train_batch_515.npy', 'train_batch_175.npy', 'train_batch_306.npy', 'train_batch_471.npy', 'train_batch_181.npy', 'train_batch_51.npy', 'train_batch_419.npy', 'train_batch_571.npy', 'train_batch_525.npy', 'train_batch_111.npy', 'train_batch_420.npy', 'train_batch_433.npy', 'train_batch_604.npy', 'train_batch_138.npy', 'train_batch_338.npy', 'train_batch_286.npy', 'train_batch_397.npy', 'train_batch_575.npy', 'train_batch_557.npy', 'train_batch_168.npy', 'train_batch_581.npy', 'train_batch_429.npy', 'train_batch_398.npy', 'train_batch_210.npy', 'train_batch_347.npy', 'train_batch_33.npy', 'train_batch_446.npy', 'train_batch_608.npy', 'train_batch_353.npy', 'train_batch_404.npy', 'train_batch_7.npy', 'train_batch_580.npy', 'train_batch_198.npy', 'train_batch_216.npy', 'train_batch_50.npy', 'train_batch_489.npy', 'train_batch_545.npy', 'train_batch_614.npy', 'train_batch_74.npy', 'train_batch_260.npy', 'train_batch_374.npy', 'train_batch_77.npy', 'train_batch_153.npy', 'train_batch_372.npy', 'train_batch_405.npy', 'train_batch_97.npy', 'train_batch_208.npy', 'train_batch_114.npy', 'train_batch_39.npy', 'train_batch_68.npy', 'train_batch_106.npy', 'train_batch_453.npy', 'train_batch_538.npy', 'train_batch_551.npy', 'train_batch_10.npy', 'train_batch_227.npy', 'train_batch_461.npy', 'train_batch_0.npy', 'train_batch_164.npy', 'train_batch_321.npy', 'train_batch_237.npy', 'train_batch_510.npy', 'train_batch_375.npy', 'train_batch_315.npy', 'train_batch_113.npy', 'train_batch_589.npy', 'train_batch_547.npy', 'train_batch_536.npy', 'train_batch_75.npy', 'train_batch_576.npy', 'train_batch_314.npy', 'train_batch_279.npy', 'train_batch_550.npy', 'train_batch_160.npy', 'train_batch_434.npy', 'train_batch_207.npy', 'train_batch_477.npy', 'train_batch_108.npy', 'train_batch_408.npy', 'train_batch_603.npy', 'train_batch_299.npy', 'train_batch_399.npy', 'train_batch_382.npy', 'train_batch_85.npy', 'train_batch_264.npy', 'train_batch_230.npy', 'train_batch_87.npy', 'train_batch_205.npy', 'train_batch_45.npy', 'train_batch_49.npy', 'train_batch_436.npy', 'train_batch_323.npy', 'train_batch_380.npy', 'train_batch_146.npy', 'train_batch_460.npy', 'train_batch_221.npy', 'train_batch_262.npy', 'train_batch_534.npy', 'train_batch_295.npy', 'train_batch_235.npy', 'train_batch_121.npy', 'train_batch_84.npy', 'train_batch_319.npy', 'train_batch_345.npy', 'train_batch_435.npy', 'train_batch_377.npy', 'train_batch_104.npy', 'train_batch_167.npy', 'train_batch_211.npy', 'train_batch_615.npy', 'train_batch_577.npy', 'train_batch_521.npy', 'train_batch_195.npy', 'train_batch_533.npy', 'train_batch_91.npy', 'train_batch_607.npy', 'train_batch_509.npy', 'train_batch_395.npy', 'train_batch_72.npy', 'train_batch_546.npy', 'train_batch_215.npy', 'train_batch_180.npy', 'train_batch_144.npy', 'train_batch_465.npy', 'train_batch_594.npy', 'train_batch_354.npy', 'train_batch_313.npy', 'train_batch_585.npy', 'train_batch_133.npy', 'train_batch_579.npy', 'train_batch_616.npy', 'train_batch_512.npy', 'train_batch_280.npy', 'train_batch_289.npy', 'train_batch_107.npy', 'train_batch_159.npy', 'train_batch_379.npy', 'train_batch_371.npy', 'train_batch_86.npy', 'train_batch_376.npy', 'train_batch_412.npy', 'train_batch_348.npy', 'train_batch_240.npy', 'train_batch_5.npy', 'train_batch_472.npy', 'train_batch_232.npy', 'train_batch_440.npy', 'train_batch_304.npy', 'train_batch_41.npy', 'train_batch_34.npy', 'train_batch_393.npy', 'train_batch_1.npy', 'train_batch_452.npy', 'train_batch_325.npy', 'train_batch_178.npy', 'train_batch_466.npy', 'train_batch_124.npy', 'train_batch_478.npy', 'train_batch_246.npy', 'train_batch_102.npy', 'train_batch_373.npy', 'train_batch_535.npy', 'train_batch_565.npy', 'train_batch_596.npy', 'train_batch_387.npy', 'train_batch_519.npy', 'train_batch_294.npy', 'train_batch_161.npy', 'train_batch_567.npy', 'train_batch_402.npy', 'train_batch_20.npy', 'train_batch_584.npy', 'train_batch_169.npy', 'train_batch_268.npy', 'train_batch_149.npy', 'train_batch_88.npy', 'train_batch_513.npy', 'train_batch_310.npy', 'train_batch_189.npy', 'train_batch_613.npy', 'train_batch_351.npy', 'train_batch_378.npy', 'train_batch_15.npy', 'train_batch_297.npy', 'train_batch_217.npy', 'train_batch_417.npy', 'train_batch_249.npy', 'train_batch_401.npy', 'train_batch_66.npy', 'train_batch_254.npy', 'train_batch_251.npy', 'train_batch_360.npy', 'train_batch_278.npy', 'train_batch_83.npy', 'train_batch_248.npy', 'train_batch_574.npy', 'train_batch_563.npy', 'train_batch_368.npy', 'train_batch_492.npy', 'train_batch_47.npy', 'train_batch_322.npy', 'train_batch_340.npy', 'train_batch_80.npy', 'train_batch_173.npy', 'train_batch_400.npy', 'train_batch_272.npy', 'train_batch_358.npy', 'train_batch_480.npy', 'train_batch_450.npy', 'train_batch_470.npy', 'train_batch_427.npy', 'train_batch_496.npy', 'train_batch_441.npy', 'train_batch_602.npy', 'train_batch_568.npy', 'train_batch_499.npy', 'train_batch_588.npy', 'train_batch_619.npy', 'train_batch_96.npy', 'train_batch_36.npy', 'train_batch_458.npy', 'train_batch_302.npy', 'train_batch_448.npy', 'train_batch_26.npy', 'train_batch_467.npy', 'train_batch_505.npy', 'train_batch_443.npy', 'train_batch_17.npy', 'train_batch_428.npy', 'train_batch_37.npy', 'train_batch_38.npy', 'train_batch_524.npy', 'train_batch_621.npy', 'train_batch_486.npy', 'train_batch_530.npy', 'train_batch_479.npy', 'train_batch_14.npy', 'train_batch_308.npy', 'train_batch_257.npy', 'train_batch_58.npy', 'train_batch_491.npy', 'train_batch_239.npy', 'train_batch_135.npy', 'train_batch_267.npy', 'train_batch_309.npy', 'train_batch_89.npy', 'train_batch_431.npy', 'train_batch_123.npy', 'train_batch_293.npy', 'train_batch_334.npy', 'train_batch_243.npy', 'train_batch_256.npy', 'train_batch_218.npy', 'train_batch_487.npy', 'train_batch_386.npy', 'train_batch_421.npy', 'train_batch_389.npy', 'train_batch_290.npy', 'train_batch_503.npy', 'train_batch_176.npy', 'train_batch_624.npy', 'train_batch_188.npy', 'train_batch_291.npy', 'train_batch_566.npy', 'train_batch_501.npy', 'train_batch_468.npy', 'train_batch_438.npy', 'train_batch_29.npy', 'train_batch_444.npy', 'train_batch_67.npy', 'train_batch_28.npy', 'train_batch_53.npy', 'train_batch_406.npy', 'train_batch_592.npy', 'train_batch_245.npy', 'train_batch_298.npy', 'train_batch_559.npy', 'train_batch_517.npy', 'train_batch_463.npy', 'train_batch_582.npy', 'train_batch_391.npy', 'train_batch_284.npy', 'train_batch_329.npy', 'train_batch_117.npy', 'train_batch_593.npy', 'train_batch_301.npy', 'train_batch_127.npy', 'train_batch_349.npy', 'train_batch_224.npy', 'train_batch_300.npy', 'train_batch_241.npy', 'train_batch_69.npy', 'train_batch_55.npy', 'train_batch_612.npy', 'train_batch_561.npy', 'train_batch_105.npy', 'train_batch_234.npy', 'train_batch_532.npy', 'train_batch_250.npy', 'train_batch_225.npy', 'train_batch_326.npy', 'train_batch_8.npy', 'train_batch_330.npy', 'train_batch_366.npy']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_dir = \"/kaggle/input/aiguard-split-data/split_data/train\"\n",
    "print(\"Train directory contents:\", os.listdir(train_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T01:27:10.547449Z",
     "iopub.status.busy": "2025-05-13T01:27:10.547256Z",
     "iopub.status.idle": "2025-05-13T01:27:10.683124Z",
     "shell.execute_reply": "2025-05-13T01:27:10.682431Z",
     "shell.execute_reply.started": "2025-05-13T01:27:10.547432Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test images shape: (20, 224, 224, 3)\n",
      "Test labels shape: (20,)\n",
      "Sample images data:\n",
      "[[[[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.00392157 0.00392157 0.        ]\n",
      "   [0.00392157 0.00392157 0.        ]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.01960784 0.01960784 0.01960784]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   [0.00392157 0.00392157 0.00392157]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   ...\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]]\n",
      "\n",
      "  [[0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   ...\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]]\n",
      "\n",
      "  [[0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   ...\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   ...\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]]\n",
      "\n",
      "  [[0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   ...\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]]\n",
      "\n",
      "  [[0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   ...\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]\n",
      "   [0.11764706 0.11764706 0.11764706]]]\n",
      "\n",
      "\n",
      " [[[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]\n",
      "\n",
      "  [[0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   ...\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]\n",
      "   [0.         0.         0.        ]]]]\n",
      "Sample labels data:\n",
      "['Abbas_Kiarostami' 'Aaron_Pena' 'Aaron_Tippin' 'Aaron_Pena' 'Aaron_Pena']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# File paths\n",
    "images_path = '/kaggle/input/aiguard-split-data/split_data/test/test_batch_0.npy/test_images_batch_0.npy'\n",
    "labels_path = '/kaggle/input/aiguard-split-data/split_data/test/test_batch_0.npy/test_labels_batch_0.npy'\n",
    "\n",
    "# Load the data\n",
    "test_images = np.load(images_path)\n",
    "test_labels = np.load(labels_path)\n",
    "\n",
    "# Check the shapes and a sample of data\n",
    "print(f\"Test images shape: {test_images.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "\n",
    "# Optional: Print out a small sample of the data\n",
    "print(f\"Sample images data:\\n{test_images[:5]}\")\n",
    "print(f\"Sample labels data:\\n{test_labels[:5]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T01:27:10.684034Z",
     "iopub.status.busy": "2025-05-13T01:27:10.683849Z",
     "iopub.status.idle": "2025-05-13T01:27:14.617379Z",
     "shell.execute_reply": "2025-05-13T01:27:14.616666Z",
     "shell.execute_reply.started": "2025-05-13T01:27:10.684018Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch images shape: (640, 224, 224, 3)\n",
      "Train batch labels shape: (640,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Function to load data in smaller batches\n",
    "def load_data_in_batches(folder_path, batch_size=10):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # List all subfolders (batches)\n",
    "    subfolders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "    \n",
    "    for subfolder in subfolders:\n",
    "        batch_path = os.path.join(folder_path, subfolder)\n",
    "        \n",
    "        # List all image and label files in the subfolder\n",
    "        image_files = [f for f in os.listdir(batch_path) if 'images_batch' in f]\n",
    "        label_files = [f for f in os.listdir(batch_path) if 'labels_batch' in f]\n",
    "        \n",
    "        # Load the batches in chunks\n",
    "        for image_file, label_file in zip(image_files, label_files):\n",
    "            image_path = os.path.join(batch_path, image_file)\n",
    "            label_path = os.path.join(batch_path, label_file)\n",
    "            \n",
    "            # Load image and label batch\n",
    "            batch_images = np.load(image_path)\n",
    "            batch_labels = np.load(label_path)\n",
    "            \n",
    "            all_images.append(batch_images)\n",
    "            all_labels.append(batch_labels)\n",
    "            \n",
    "            # If we have loaded enough data, yield the batch and clear memory\n",
    "            if len(all_images) >= batch_size:\n",
    "                yield np.concatenate(all_images, axis=0), np.concatenate(all_labels, axis=0)\n",
    "                all_images = []\n",
    "                all_labels = []\n",
    "    \n",
    "    # If there are remaining images, yield them too\n",
    "    if len(all_images) > 0:\n",
    "        yield np.concatenate(all_images, axis=0), np.concatenate(all_labels, axis=0)\n",
    "\n",
    "# Function to load data from train, val, and test folders\n",
    "def load_all_data(train_folder, val_folder, test_folder, batch_size=10):\n",
    "    train_data = load_data_in_batches(train_folder, batch_size)\n",
    "    val_data = load_data_in_batches(val_folder, batch_size)\n",
    "    test_data = load_data_in_batches(test_folder, batch_size)\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Define paths\n",
    "train_folder = '/kaggle/input/aiguard-split-data/split_data/train'\n",
    "val_folder = '/kaggle/input/aiguard-split-data/split_data/val'\n",
    "test_folder = '/kaggle/input/aiguard-split-data/split_data/test'\n",
    "\n",
    "# Load data in batches\n",
    "train_data, val_data, test_data = load_all_data(train_folder, val_folder, test_folder, batch_size=10)\n",
    "\n",
    "# Example of how to access the data in batches\n",
    "for batch_images, batch_labels in train_data:\n",
    "    print(f\"Train batch images shape: {batch_images.shape}\")\n",
    "    print(f\"Train batch labels shape: {batch_labels.shape}\")\n",
    "    break  # Only print the first batch for inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T01:27:14.618460Z",
     "iopub.status.busy": "2025-05-13T01:27:14.618215Z",
     "iopub.status.idle": "2025-05-13T01:27:14.790387Z",
     "shell.execute_reply": "2025-05-13T01:27:14.789035Z",
     "shell.execute_reply.started": "2025-05-13T01:27:14.618431Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|                                                      | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IsADirectoryError",
     "evalue": "Caught IsADirectoryError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_69/4001607439.py\", line 19, in __getitem__\n    data = np.load(file_path, allow_pickle=True).item()  # Assuming saved as dict with 'image' and 'label'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/numpy/lib/npyio.py\", line 427, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIsADirectoryError: [Errno 21] Is a directory: '/kaggle/input/aiguard-split-data/split_data/train/train_batch_28.npy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_69/725559067.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrunning_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Training Epoch {epoch + 1}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: Caught IsADirectoryError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_69/4001607439.py\", line 19, in __getitem__\n    data = np.load(file_path, allow_pickle=True).item()  # Assuming saved as dict with 'image' and 'label'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/numpy/lib/npyio.py\", line 427, in load\n    fid = stack.enter_context(open(os_fspath(file), \"rb\"))\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\nIsADirectoryError: [Errno 21] Is a directory: '/kaggle/input/aiguard-split-data/split_data/train/train_batch_28.npy'\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss, correct_preds, total_preds = 0.0, 0, 0\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", ncols=100):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_preds += (predicted == labels).sum().item()\n",
    "        total_preds += labels.size(0)\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct_preds / total_preds * 100\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f\"Training - Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, correct_preds, total_preds = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validating\", ncols=100):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = correct_preds / total_preds * 100\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"Epoch completed in {epoch_time:.2f} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-13T01:27:14.791526Z",
     "iopub.status.idle": "2025-05-13T01:27:14.791831Z",
     "shell.execute_reply": "2025-05-13T01:27:14.791697Z",
     "shell.execute_reply.started": "2025-05-13T01:27:14.791677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Plotting Loss and Accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Train Loss\", color=\"blue\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\", color=\"red\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss vs Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_accuracies, label=\"Train Accuracy\", color=\"blue\")\n",
    "plt.plot(range(1, num_epochs + 1), val_accuracies, label=\"Validation Accuracy\", color=\"red\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Accuracy vs Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7402081,
     "sourceId": 11788764,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "cv_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
